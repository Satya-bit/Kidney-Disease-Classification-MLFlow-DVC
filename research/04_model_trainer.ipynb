{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\satya\\\\Documents\\\\data_science_roadmap\\\\Deep_Learning\\\\Project\\\\Kidney-Disease-Classification-MLFlow-DVC\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\satya\\\\Documents\\\\data_science_roadmap\\\\Deep_Learning\\\\Project\\\\Kidney-Disease-Classification-MLFlow-DVC'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entity\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path\n",
    "    updated_base_model_path: Path\n",
    "    training_data: Path\n",
    "    validation_data: Path\n",
    "    params_epochs: int\n",
    "    params_batch_size: int\n",
    "    params_is_augmentation: bool\n",
    "    params_image_size: list\n",
    "    params_learning_rate: float\n",
    "    params_monitor: str\n",
    "    params_patience: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config manager\n",
    "from src.cnnClassifier.constants import *\n",
    "from src.cnnClassifier.utils.common import read_yaml, create_directories\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "            training = self.config.training\n",
    "            prepare_base_model = self.config.prepare_base_model\n",
    "            params = self.params\n",
    "            training_data =os.path.join(self.config.data_transformation.split, \"train\")\n",
    "            validation_data =os.path.join(self.config.data_transformation.split, \"test\")\n",
    "            create_directories([\n",
    "                Path(training.root_dir)\n",
    "            ])\n",
    "            training_config = TrainingConfig(\n",
    "                root_dir=Path(training.root_dir),\n",
    "                trained_model_path=Path(training.trained_model_path),\n",
    "                updated_base_model_path=Path(prepare_base_model.updated_base_model_path),\n",
    "                # base_model_path=Path(prepare_base_model.base_model_path),\n",
    "                training_data=Path(training_data),\n",
    "                validation_data=Path(validation_data),\n",
    "                params_epochs=params.EPOCHS,\n",
    "                params_batch_size=params.BATCH_SIZE,\n",
    "                params_is_augmentation=params.AUGMENTATION,\n",
    "                params_image_size=params.IMAGE_SIZE,\n",
    "                params_learning_rate=params.LEARNING_RATE,\n",
    "                params_monitor=params.MONITOR,\n",
    "                params_patience=params.PATIENCE\n",
    "            )\n",
    "\n",
    "            return training_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#componenets\n",
    "import os\n",
    "import urllib.request as request\n",
    "from zipfile import ZipFile\n",
    "import tensorflow as tf\n",
    "import time\n",
    "tf.config.run_functions_eagerly(True)  # Force eager execution globally\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Training:\n",
    "    def __init__(self,config: TrainingConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def get_base_model(self): #To get the updted model\n",
    "        self.model=tf.keras.models.load_model(\n",
    "            self.config.updated_base_model_path\n",
    "            # self.config.base_model_path\n",
    "        )\n",
    "        self.model.compile(\n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=self.config.params_learning_rate),\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "        \n",
    "    def train_valid_generator(self): #Function for splitting the train and validation data with preprocessing\n",
    "\n",
    "        datagenerator_kwargs = dict(\n",
    "            # rescale = 1./255, #Common normslisation step\n",
    "            # validation_split=0.20\n",
    "            preprocessing_function=preprocess_input\n",
    "\n",
    "        )\n",
    "\n",
    "        dataflow_kwargs = dict(\n",
    "            target_size=self.config.params_image_size[:-1], #Resizing the image to 224x224\n",
    "            batch_size=self.config.params_batch_size,\n",
    "            interpolation=\"bilinear\"  #A technique used for resizing the image. It uses a weighted average of the 4 nearest pixels.\n",
    "        )\n",
    "\n",
    "#preparing the validation data\n",
    "        valid_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            **datagenerator_kwargs\n",
    "        ) #This will take 10% from normal and 10% from tumor directories roughly\n",
    "\n",
    "        self.valid_generator = valid_datagenerator.flow_from_directory(\n",
    "            directory=self.config.validation_data,\n",
    "            shuffle=False,\n",
    "            **dataflow_kwargs\n",
    "        ) #Resized image of 224x224 in validation set(20%, 10 % from each class)\n",
    "\n",
    "#preparing the training data\n",
    "        # if self.config.params_is_augmentation: #If augmentation is kept true it will try to transform the image. Note it won't be adding data to the training set. Just each batch will get randomly transformed as per the arguments passed\n",
    "        train_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            rotation_range=30,  # Wider rotation range\n",
    "            vertical_flip=True,\n",
    "            horizontal_flip=True,\n",
    "            width_shift_range=0.1,  # Slightly larger shift range\n",
    "            height_shift_range=0.1,\n",
    "            shear_range=0.1,\n",
    "            zoom_range=0.3,  # Increased zoom range\n",
    "            brightness_range=[0.3, 1.7],  # Increased brightness range\n",
    "            **datagenerator_kwargs\n",
    "        )\n",
    "        # else:\n",
    "        #     train_datagenerator = valid_datagenerator #If augemnetation is kept false it will not try to transform the image. Just rescaling. \n",
    "\n",
    "#Taking 80% of data from normal and tumor directories roughly. Doing data augmentation. Also shuffling to avoid memorization while training\n",
    "        self.train_generator = train_datagenerator.flow_from_directory(\n",
    "            directory=self.config.training_data,\n",
    "            shuffle=True,\n",
    "            **dataflow_kwargs\n",
    "        )\n",
    "        \n",
    "        \n",
    "    @staticmethod #Saving the train model\n",
    "    def save_model(path: Path, model: tf.keras.Model):\n",
    "        model.save(path)\n",
    "\n",
    "\n",
    "#Iteration*batch=data points*epochs\n",
    "\n",
    "    \n",
    "    def train(self): #Function to train the model\n",
    "        # self.steps_per_epoch = math.ceil(self.train_generator.samples / self.train_generator.batch_size) #This is number of iteration per epoch. The number of times the model parameters will be updated per epoch.\n",
    "        # self.validation_steps = math.ceil(self.valid_generator.samples / self.valid_generator.batch_size) #After every iteration the model will be validated. \n",
    "        #For example if the validation has 100 samples and the batch size is 10 the evaluation will be done for 100/10=10 samples. A record of correct samples is kept. in these 10 samples. then afterwards its added together in the end.\n",
    "        early_stopping = EarlyStopping(\n",
    "            patience=self.config.params_patience,\n",
    "            monitor=self.config.params_monitor\n",
    "        )\n",
    "        self.model.fit(\n",
    "            self.train_generator,\n",
    "            epochs=self.config.params_epochs,\n",
    "            # steps_per_epoch=self.steps_per_epoch,\n",
    "            # validation_steps=self.validation_steps,\n",
    "            validation_data=self.valid_generator,\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "\n",
    "        self.save_model(\n",
    "            path=self.config.trained_model_path,\n",
    "            model=self.model\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-02 10:21:12,877: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-04-02 10:21:12,880: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-04-02 10:21:12,881: INFO: common: created directory at: artifacts]\n",
      "[2025-04-02 10:21:12,882: INFO: common: created directory at: artifacts\\training]\n",
      "[2025-04-02 10:21:13,227: WARNING: saving_utils: Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.]\n",
      "Found 1056 images belonging to 2 classes.\n",
      "Found 4231 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\satya\\Documents\\data_science_roadmap\\Deep_Learning\\Project\\Kidney-Disease-Classification-MLFlow-DVC\\venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "c:\\Users\\satya\\Documents\\data_science_roadmap\\Deep_Learning\\Project\\Kidney-Disease-Classification-MLFlow-DVC\\venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1704s\u001b[0m 6s/step - accuracy: 0.6792 - loss: 5.0359 - val_accuracy: 0.8826 - val_loss: 2.4238\n",
      "[2025-04-02 10:49:37,919: WARNING: saving_api: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. ]\n"
     ]
    }
   ],
   "source": [
    "#pipeline\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    training_config = config.get_training_config()\n",
    "    training = Training(config=training_config)\n",
    "    training.get_base_model()\n",
    "    training.train_valid_generator()\n",
    "    training.train()\n",
    "\n",
    "except Exception as e:\n",
    "    raise e\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
